//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_90a
.address_size 64

	// .globl	_Z4testI6__halfS0_S0_EvPT_PT0_PT1_
.extern .shared .align 16 .b8 smem[];

.visible .entry _Z4testI6__halfS0_S0_EvPT_PT0_PT1_(
	.param .u64 _Z4testI6__halfS0_S0_EvPT_PT0_PT1__param_0,
	.param .u64 _Z4testI6__halfS0_S0_EvPT_PT0_PT1__param_1,
	.param .u64 _Z4testI6__halfS0_S0_EvPT_PT0_PT1__param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<69>;
	.reg .b32 	%r<155>;
	.reg .b64 	%rd<143>;


	ld.param.u64 	%rd3, [_Z4testI6__halfS0_S0_EvPT_PT0_PT1__param_0];
	ld.param.u64 	%rd4, [_Z4testI6__halfS0_S0_EvPT_PT0_PT1__param_1];
	ld.param.u64 	%rd5, [_Z4testI6__halfS0_S0_EvPT_PT0_PT1__param_2];
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 1023;
	@%p1 bra 	$L__BB0_3;

	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, smem;
	mov.u32 	%r153, %r1;

$L__BB0_2:
	mul.wide.s32 	%rd6, %r153, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs1, [%rd7];
	shl.b32 	%r8, %r153, 1;
	add.s32 	%r10, %r9, %r8;
	st.shared.u16 	[%r10], %rs1;
	add.s32 	%r153, %r153, %r2;
	setp.lt.s32 	%p2, %r153, 1024;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	setp.gt.s32 	%p3, %r1, 2047;
	@%p3 bra 	$L__BB0_6;

	mov.u32 	%r5, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r12, smem;
	mov.u32 	%r154, %r1;

$L__BB0_5:
	mul.wide.s32 	%rd8, %r154, 2;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.u16 	%rs2, [%rd9];
	shl.b32 	%r11, %r154, 1;
	add.s32 	%r13, %r12, %r11;
	st.shared.u16 	[%r13+2048], %rs2;
	add.s32 	%r154, %r154, %r5;
	setp.lt.s32 	%p4, %r154, 2048;
	@%p4 bra 	$L__BB0_5;

$L__BB0_6:
	cvta.to.global.u64 	%rd12, %rd5;
	bar.sync 	0;
	mov.u32 	%r18, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs3, %r18;
	// end inline asm
	mov.u16 	%rs4, 0;
	mov.b32 	%r17, {%rs3, %rs4};
	// begin inline asm
	// end inline asm
	// begin inline asm
	wgmma.fence.sync.aligned;

	// end inline asm
	mov.u32 	%r84, smem;
	shr.u32 	%r85, %r84, 4;
	and.b32  	%r86, %r85, 16383;
	cvt.u64.u32 	%rd13, %r86;
	or.b64  	%rd10, %rd13, 68720001024;
	add.s32 	%r87, %r84, 2048;
	shr.u32 	%r88, %r87, 4;
	and.b32  	%r89, %r88, 16383;
	cvt.u64.u32 	%rd14, %r89;
	or.b64  	%rd11, %rd14, 34368126976;
	mov.u32 	%r19, %r18;
	mov.u32 	%r20, %r18;
	mov.u32 	%r21, %r18;
	mov.u32 	%r22, %r18;
	mov.u32 	%r23, %r18;
	mov.u32 	%r24, %r18;
	mov.u32 	%r25, %r18;
	mov.u32 	%r26, %r18;
	mov.u32 	%r27, %r18;
	mov.u32 	%r28, %r18;
	mov.u32 	%r29, %r18;
	mov.u32 	%r30, %r18;
	mov.u32 	%r31, %r18;
	mov.u32 	%r32, %r18;
	mov.u32 	%r33, %r18;
	mov.u32 	%r34, %r18;
	mov.u32 	%r35, %r18;
	mov.u32 	%r36, %r18;
	mov.u32 	%r37, %r18;
	mov.u32 	%r38, %r18;
	mov.u32 	%r39, %r18;
	mov.u32 	%r40, %r18;
	mov.u32 	%r41, %r18;
	mov.u32 	%r42, %r18;
	mov.u32 	%r43, %r18;
	mov.u32 	%r44, %r18;
	mov.u32 	%r45, %r18;
	mov.u32 	%r46, %r18;
	mov.u32 	%r47, %r18;
	mov.u32 	%r48, %r18;
	// begin inline asm
	{
.reg .pred p;
setp.ne.b32 p, %r48, 0;
wgmma.mma_async.sync.aligned.m64n128k16.f16.f16.f16 {%r17,  %r18,  %r19,  %r20,  %r21,  %r22,  %r23,  %r24,   %r25,  %r26,  %r27, %r28, %r29, %r30, %r31, %r32,  %r33, %r34, %r35, %r36, %r37, %r38, %r39, %r40,  %r41, %r42, %r43, %r44, %r45, %r46, %r47, %r48}, %rd10, %rd11, p,   1, 1, 0, 1;
}

	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;

	// end inline asm
	// begin inline asm
	wgmma.wait_group.sync.aligned 0;

	// end inline asm
	// begin inline asm
	// end inline asm
	mul.wide.u32 	%rd15, %r1, 2;
	add.s64 	%rd16, %rd12, %rd15;
	add.s32 	%r90, %r1, 128;
	mov.b32 	{%rs5, %rs6}, %r17;
	mul.wide.u32 	%rd17, %r90, 2;
	add.s64 	%rd18, %rd12, %rd17;
	st.global.u16 	[%rd16], %rs5;
	st.global.u16 	[%rd18], %rs6;
	add.s32 	%r91, %r1, 256;
	mul.wide.u32 	%rd19, %r91, 2;
	add.s64 	%rd20, %rd12, %rd19;
	mov.b32 	{%rs7, %rs8}, %r18;
	st.global.u16 	[%rd20], %rs7;
	add.s32 	%r92, %r1, 384;
	mul.wide.u32 	%rd21, %r92, 2;
	add.s64 	%rd22, %rd12, %rd21;
	st.global.u16 	[%rd22], %rs8;
	add.s32 	%r93, %r1, 512;
	mul.wide.u32 	%rd23, %r93, 2;
	add.s64 	%rd24, %rd12, %rd23;
	mov.b32 	{%rs9, %rs10}, %r19;
	st.global.u16 	[%rd24], %rs9;
	add.s32 	%r94, %r1, 640;
	mul.wide.u32 	%rd25, %r94, 2;
	add.s64 	%rd26, %rd12, %rd25;
	st.global.u16 	[%rd26], %rs10;
	add.s32 	%r95, %r1, 768;
	mul.wide.u32 	%rd27, %r95, 2;
	add.s64 	%rd28, %rd12, %rd27;
	mov.b32 	{%rs11, %rs12}, %r20;
	st.global.u16 	[%rd28], %rs11;
	add.s32 	%r96, %r1, 896;
	mul.wide.u32 	%rd29, %r96, 2;
	add.s64 	%rd30, %rd12, %rd29;
	st.global.u16 	[%rd30], %rs12;
	add.s32 	%r97, %r1, 1024;
	mul.wide.u32 	%rd31, %r97, 2;
	add.s64 	%rd32, %rd12, %rd31;
	mov.b32 	{%rs13, %rs14}, %r21;
	st.global.u16 	[%rd32], %rs13;
	add.s32 	%r98, %r1, 1152;
	mul.wide.u32 	%rd33, %r98, 2;
	add.s64 	%rd34, %rd12, %rd33;
	st.global.u16 	[%rd34], %rs14;
	add.s32 	%r99, %r1, 1280;
	mul.wide.u32 	%rd35, %r99, 2;
	add.s64 	%rd36, %rd12, %rd35;
	mov.b32 	{%rs15, %rs16}, %r22;
	st.global.u16 	[%rd36], %rs15;
	add.s32 	%r100, %r1, 1408;
	mul.wide.u32 	%rd37, %r100, 2;
	add.s64 	%rd38, %rd12, %rd37;
	st.global.u16 	[%rd38], %rs16;
	add.s32 	%r101, %r1, 1536;
	mul.wide.u32 	%rd39, %r101, 2;
	add.s64 	%rd40, %rd12, %rd39;
	mov.b32 	{%rs17, %rs18}, %r23;
	st.global.u16 	[%rd40], %rs17;
	add.s32 	%r102, %r1, 1664;
	mul.wide.u32 	%rd41, %r102, 2;
	add.s64 	%rd42, %rd12, %rd41;
	st.global.u16 	[%rd42], %rs18;
	add.s32 	%r103, %r1, 1792;
	mul.wide.u32 	%rd43, %r103, 2;
	add.s64 	%rd44, %rd12, %rd43;
	mov.b32 	{%rs19, %rs20}, %r24;
	st.global.u16 	[%rd44], %rs19;
	add.s32 	%r104, %r1, 1920;
	mul.wide.u32 	%rd45, %r104, 2;
	add.s64 	%rd46, %rd12, %rd45;
	st.global.u16 	[%rd46], %rs20;
	add.s32 	%r105, %r1, 2048;
	mul.wide.u32 	%rd47, %r105, 2;
	add.s64 	%rd48, %rd12, %rd47;
	mov.b32 	{%rs21, %rs22}, %r25;
	st.global.u16 	[%rd48], %rs21;
	add.s32 	%r106, %r1, 2176;
	mul.wide.u32 	%rd49, %r106, 2;
	add.s64 	%rd50, %rd12, %rd49;
	st.global.u16 	[%rd50], %rs22;
	add.s32 	%r107, %r1, 2304;
	mul.wide.u32 	%rd51, %r107, 2;
	add.s64 	%rd52, %rd12, %rd51;
	mov.b32 	{%rs23, %rs24}, %r26;
	st.global.u16 	[%rd52], %rs23;
	add.s32 	%r108, %r1, 2432;
	mul.wide.u32 	%rd53, %r108, 2;
	add.s64 	%rd54, %rd12, %rd53;
	st.global.u16 	[%rd54], %rs24;
	add.s32 	%r109, %r1, 2560;
	mul.wide.u32 	%rd55, %r109, 2;
	add.s64 	%rd56, %rd12, %rd55;
	mov.b32 	{%rs25, %rs26}, %r27;
	st.global.u16 	[%rd56], %rs25;
	add.s32 	%r110, %r1, 2688;
	mul.wide.u32 	%rd57, %r110, 2;
	add.s64 	%rd58, %rd12, %rd57;
	st.global.u16 	[%rd58], %rs26;
	add.s32 	%r111, %r1, 2816;
	mul.wide.u32 	%rd59, %r111, 2;
	add.s64 	%rd60, %rd12, %rd59;
	mov.b32 	{%rs27, %rs28}, %r28;
	st.global.u16 	[%rd60], %rs27;
	add.s32 	%r112, %r1, 2944;
	mul.wide.u32 	%rd61, %r112, 2;
	add.s64 	%rd62, %rd12, %rd61;
	st.global.u16 	[%rd62], %rs28;
	add.s32 	%r113, %r1, 3072;
	mul.wide.u32 	%rd63, %r113, 2;
	add.s64 	%rd64, %rd12, %rd63;
	mov.b32 	{%rs29, %rs30}, %r29;
	st.global.u16 	[%rd64], %rs29;
	add.s32 	%r114, %r1, 3200;
	mul.wide.u32 	%rd65, %r114, 2;
	add.s64 	%rd66, %rd12, %rd65;
	st.global.u16 	[%rd66], %rs30;
	add.s32 	%r115, %r1, 3328;
	mul.wide.u32 	%rd67, %r115, 2;
	add.s64 	%rd68, %rd12, %rd67;
	mov.b32 	{%rs31, %rs32}, %r30;
	st.global.u16 	[%rd68], %rs31;
	add.s32 	%r116, %r1, 3456;
	mul.wide.u32 	%rd69, %r116, 2;
	add.s64 	%rd70, %rd12, %rd69;
	st.global.u16 	[%rd70], %rs32;
	add.s32 	%r117, %r1, 3584;
	mul.wide.u32 	%rd71, %r117, 2;
	add.s64 	%rd72, %rd12, %rd71;
	mov.b32 	{%rs33, %rs34}, %r31;
	st.global.u16 	[%rd72], %rs33;
	add.s32 	%r118, %r1, 3712;
	mul.wide.u32 	%rd73, %r118, 2;
	add.s64 	%rd74, %rd12, %rd73;
	st.global.u16 	[%rd74], %rs34;
	add.s32 	%r119, %r1, 3840;
	mul.wide.u32 	%rd75, %r119, 2;
	add.s64 	%rd76, %rd12, %rd75;
	mov.b32 	{%rs35, %rs36}, %r32;
	st.global.u16 	[%rd76], %rs35;
	add.s32 	%r120, %r1, 3968;
	mul.wide.u32 	%rd77, %r120, 2;
	add.s64 	%rd78, %rd12, %rd77;
	st.global.u16 	[%rd78], %rs36;
	add.s32 	%r121, %r1, 4096;
	mul.wide.u32 	%rd79, %r121, 2;
	add.s64 	%rd80, %rd12, %rd79;
	mov.b32 	{%rs37, %rs38}, %r33;
	st.global.u16 	[%rd80], %rs37;
	add.s32 	%r122, %r1, 4224;
	mul.wide.u32 	%rd81, %r122, 2;
	add.s64 	%rd82, %rd12, %rd81;
	st.global.u16 	[%rd82], %rs38;
	add.s32 	%r123, %r1, 4352;
	mul.wide.u32 	%rd83, %r123, 2;
	add.s64 	%rd84, %rd12, %rd83;
	mov.b32 	{%rs39, %rs40}, %r34;
	st.global.u16 	[%rd84], %rs39;
	add.s32 	%r124, %r1, 4480;
	mul.wide.u32 	%rd85, %r124, 2;
	add.s64 	%rd86, %rd12, %rd85;
	st.global.u16 	[%rd86], %rs40;
	add.s32 	%r125, %r1, 4608;
	mul.wide.u32 	%rd87, %r125, 2;
	add.s64 	%rd88, %rd12, %rd87;
	mov.b32 	{%rs41, %rs42}, %r35;
	st.global.u16 	[%rd88], %rs41;
	add.s32 	%r126, %r1, 4736;
	mul.wide.u32 	%rd89, %r126, 2;
	add.s64 	%rd90, %rd12, %rd89;
	st.global.u16 	[%rd90], %rs42;
	add.s32 	%r127, %r1, 4864;
	mul.wide.u32 	%rd91, %r127, 2;
	add.s64 	%rd92, %rd12, %rd91;
	mov.b32 	{%rs43, %rs44}, %r36;
	st.global.u16 	[%rd92], %rs43;
	add.s32 	%r128, %r1, 4992;
	mul.wide.u32 	%rd93, %r128, 2;
	add.s64 	%rd94, %rd12, %rd93;
	st.global.u16 	[%rd94], %rs44;
	add.s32 	%r129, %r1, 5120;
	mul.wide.u32 	%rd95, %r129, 2;
	add.s64 	%rd96, %rd12, %rd95;
	mov.b32 	{%rs45, %rs46}, %r37;
	st.global.u16 	[%rd96], %rs45;
	add.s32 	%r130, %r1, 5248;
	mul.wide.u32 	%rd97, %r130, 2;
	add.s64 	%rd98, %rd12, %rd97;
	st.global.u16 	[%rd98], %rs46;
	add.s32 	%r131, %r1, 5376;
	mul.wide.u32 	%rd99, %r131, 2;
	add.s64 	%rd100, %rd12, %rd99;
	mov.b32 	{%rs47, %rs48}, %r38;
	st.global.u16 	[%rd100], %rs47;
	add.s32 	%r132, %r1, 5504;
	mul.wide.u32 	%rd101, %r132, 2;
	add.s64 	%rd102, %rd12, %rd101;
	st.global.u16 	[%rd102], %rs48;
	add.s32 	%r133, %r1, 5632;
	mul.wide.u32 	%rd103, %r133, 2;
	add.s64 	%rd104, %rd12, %rd103;
	mov.b32 	{%rs49, %rs50}, %r39;
	st.global.u16 	[%rd104], %rs49;
	add.s32 	%r134, %r1, 5760;
	mul.wide.u32 	%rd105, %r134, 2;
	add.s64 	%rd106, %rd12, %rd105;
	st.global.u16 	[%rd106], %rs50;
	add.s32 	%r135, %r1, 5888;
	mul.wide.u32 	%rd107, %r135, 2;
	add.s64 	%rd108, %rd12, %rd107;
	mov.b32 	{%rs51, %rs52}, %r40;
	st.global.u16 	[%rd108], %rs51;
	add.s32 	%r136, %r1, 6016;
	mul.wide.u32 	%rd109, %r136, 2;
	add.s64 	%rd110, %rd12, %rd109;
	st.global.u16 	[%rd110], %rs52;
	add.s32 	%r137, %r1, 6144;
	mul.wide.u32 	%rd111, %r137, 2;
	add.s64 	%rd112, %rd12, %rd111;
	mov.b32 	{%rs53, %rs54}, %r41;
	st.global.u16 	[%rd112], %rs53;
	add.s32 	%r138, %r1, 6272;
	mul.wide.u32 	%rd113, %r138, 2;
	add.s64 	%rd114, %rd12, %rd113;
	st.global.u16 	[%rd114], %rs54;
	add.s32 	%r139, %r1, 6400;
	mul.wide.u32 	%rd115, %r139, 2;
	add.s64 	%rd116, %rd12, %rd115;
	mov.b32 	{%rs55, %rs56}, %r42;
	st.global.u16 	[%rd116], %rs55;
	add.s32 	%r140, %r1, 6528;
	mul.wide.u32 	%rd117, %r140, 2;
	add.s64 	%rd118, %rd12, %rd117;
	st.global.u16 	[%rd118], %rs56;
	add.s32 	%r141, %r1, 6656;
	mul.wide.u32 	%rd119, %r141, 2;
	add.s64 	%rd120, %rd12, %rd119;
	mov.b32 	{%rs57, %rs58}, %r43;
	st.global.u16 	[%rd120], %rs57;
	add.s32 	%r142, %r1, 6784;
	mul.wide.u32 	%rd121, %r142, 2;
	add.s64 	%rd122, %rd12, %rd121;
	st.global.u16 	[%rd122], %rs58;
	add.s32 	%r143, %r1, 6912;
	mul.wide.u32 	%rd123, %r143, 2;
	add.s64 	%rd124, %rd12, %rd123;
	mov.b32 	{%rs59, %rs60}, %r44;
	st.global.u16 	[%rd124], %rs59;
	add.s32 	%r144, %r1, 7040;
	mul.wide.u32 	%rd125, %r144, 2;
	add.s64 	%rd126, %rd12, %rd125;
	st.global.u16 	[%rd126], %rs60;
	add.s32 	%r145, %r1, 7168;
	mul.wide.u32 	%rd127, %r145, 2;
	add.s64 	%rd128, %rd12, %rd127;
	mov.b32 	{%rs61, %rs62}, %r45;
	st.global.u16 	[%rd128], %rs61;
	add.s32 	%r146, %r1, 7296;
	mul.wide.u32 	%rd129, %r146, 2;
	add.s64 	%rd130, %rd12, %rd129;
	st.global.u16 	[%rd130], %rs62;
	add.s32 	%r147, %r1, 7424;
	mul.wide.u32 	%rd131, %r147, 2;
	add.s64 	%rd132, %rd12, %rd131;
	mov.b32 	{%rs63, %rs64}, %r46;
	st.global.u16 	[%rd132], %rs63;
	add.s32 	%r148, %r1, 7552;
	mul.wide.u32 	%rd133, %r148, 2;
	add.s64 	%rd134, %rd12, %rd133;
	st.global.u16 	[%rd134], %rs64;
	add.s32 	%r149, %r1, 7680;
	mul.wide.u32 	%rd135, %r149, 2;
	add.s64 	%rd136, %rd12, %rd135;
	mov.b32 	{%rs65, %rs66}, %r47;
	st.global.u16 	[%rd136], %rs65;
	add.s32 	%r150, %r1, 7808;
	mul.wide.u32 	%rd137, %r150, 2;
	add.s64 	%rd138, %rd12, %rd137;
	st.global.u16 	[%rd138], %rs66;
	add.s32 	%r151, %r1, 7936;
	mul.wide.u32 	%rd139, %r151, 2;
	add.s64 	%rd140, %rd12, %rd139;
	mov.b32 	{%rs67, %rs68}, %r48;
	st.global.u16 	[%rd140], %rs67;
	add.s32 	%r152, %r1, 8064;
	mul.wide.u32 	%rd141, %r152, 2;
	add.s64 	%rd142, %rd12, %rd141;
	st.global.u16 	[%rd142], %rs68;
	ret;

}
	// .globl	_Z9referenceI6__halfS0_S0_EvPT_PT0_PT1_
.visible .entry _Z9referenceI6__halfS0_S0_EvPT_PT0_PT1_(
	.param .u64 _Z9referenceI6__halfS0_S0_EvPT_PT0_PT1__param_0,
	.param .u64 _Z9referenceI6__halfS0_S0_EvPT_PT0_PT1__param_1,
	.param .u64 _Z9referenceI6__halfS0_S0_EvPT_PT0_PT1__param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<97>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd5, [_Z9referenceI6__halfS0_S0_EvPT_PT0_PT1__param_0];
	ld.param.u64 	%rd6, [_Z9referenceI6__halfS0_S0_EvPT_PT0_PT1__param_1];
	ld.param.u64 	%rd7, [_Z9referenceI6__halfS0_S0_EvPT_PT0_PT1__param_2];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r11, 0;
	mov.u32 	%r2, %ntid.x;

$L__BB1_1:
	setp.gt.s32 	%p1, %r1, 127;
	@%p1 bra 	$L__BB1_4;

	shl.b32 	%r9, %r11, 4;
	shl.b32 	%r4, %r11, 7;
	mul.wide.s32 	%rd8, %r9, 2;
	add.s64 	%rd4, %rd3, %rd8;
	mov.u32 	%r12, %r1;

$L__BB1_3:
	add.s32 	%r10, %r12, %r4;
	mul.wide.s32 	%rd9, %r10, 2;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u16 	%rs2, [%rd4];
	mul.wide.s32 	%rd11, %r12, 2;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.u16 	%rs3, [%rd12];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	ld.global.u16 	%rs5, [%rd10];
	// begin inline asm
	{add.f16 %rs4,%rs5,%rs1;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs4;
	ld.global.u16 	%rs8, [%rd4+2];
	ld.global.u16 	%rs9, [%rd12+256];
	// begin inline asm
	{mul.f16 %rs7,%rs8,%rs9;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs10,%rs4,%rs7;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs10;
	ld.global.u16 	%rs14, [%rd4+4];
	ld.global.u16 	%rs15, [%rd12+512];
	// begin inline asm
	{mul.f16 %rs13,%rs14,%rs15;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs16,%rs10,%rs13;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs16;
	ld.global.u16 	%rs20, [%rd4+6];
	ld.global.u16 	%rs21, [%rd12+768];
	// begin inline asm
	{mul.f16 %rs19,%rs20,%rs21;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs22,%rs16,%rs19;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs22;
	ld.global.u16 	%rs26, [%rd4+8];
	ld.global.u16 	%rs27, [%rd12+1024];
	// begin inline asm
	{mul.f16 %rs25,%rs26,%rs27;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs28,%rs22,%rs25;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs28;
	ld.global.u16 	%rs32, [%rd4+10];
	ld.global.u16 	%rs33, [%rd12+1280];
	// begin inline asm
	{mul.f16 %rs31,%rs32,%rs33;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs34,%rs28,%rs31;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs34;
	ld.global.u16 	%rs38, [%rd4+12];
	ld.global.u16 	%rs39, [%rd12+1536];
	// begin inline asm
	{mul.f16 %rs37,%rs38,%rs39;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs40,%rs34,%rs37;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs40;
	ld.global.u16 	%rs44, [%rd4+14];
	ld.global.u16 	%rs45, [%rd12+1792];
	// begin inline asm
	{mul.f16 %rs43,%rs44,%rs45;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs46,%rs40,%rs43;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs46;
	ld.global.u16 	%rs50, [%rd4+16];
	ld.global.u16 	%rs51, [%rd12+2048];
	// begin inline asm
	{mul.f16 %rs49,%rs50,%rs51;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs52,%rs46,%rs49;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs52;
	ld.global.u16 	%rs56, [%rd4+18];
	ld.global.u16 	%rs57, [%rd12+2304];
	// begin inline asm
	{mul.f16 %rs55,%rs56,%rs57;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs58,%rs52,%rs55;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs58;
	ld.global.u16 	%rs62, [%rd4+20];
	ld.global.u16 	%rs63, [%rd12+2560];
	// begin inline asm
	{mul.f16 %rs61,%rs62,%rs63;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs64,%rs58,%rs61;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs64;
	ld.global.u16 	%rs68, [%rd4+22];
	ld.global.u16 	%rs69, [%rd12+2816];
	// begin inline asm
	{mul.f16 %rs67,%rs68,%rs69;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs70,%rs64,%rs67;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs70;
	ld.global.u16 	%rs74, [%rd4+24];
	ld.global.u16 	%rs75, [%rd12+3072];
	// begin inline asm
	{mul.f16 %rs73,%rs74,%rs75;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs76,%rs70,%rs73;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs76;
	ld.global.u16 	%rs80, [%rd4+26];
	ld.global.u16 	%rs81, [%rd12+3328];
	// begin inline asm
	{mul.f16 %rs79,%rs80,%rs81;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs82,%rs76,%rs79;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs82;
	ld.global.u16 	%rs86, [%rd4+28];
	ld.global.u16 	%rs87, [%rd12+3584];
	// begin inline asm
	{mul.f16 %rs85,%rs86,%rs87;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs88,%rs82,%rs85;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs88;
	ld.global.u16 	%rs92, [%rd4+30];
	ld.global.u16 	%rs93, [%rd12+3840];
	// begin inline asm
	{mul.f16 %rs91,%rs92,%rs93;
}
	// end inline asm
	// begin inline asm
	{add.f16 %rs94,%rs88,%rs91;
}
	// end inline asm
	st.global.u16 	[%rd10], %rs94;
	add.s32 	%r12, %r12, %r2;
	setp.lt.s32 	%p2, %r12, 128;
	@%p2 bra 	$L__BB1_3;

$L__BB1_4:
	add.s32 	%r11, %r11, 1;
	setp.lt.u32 	%p3, %r11, 64;
	@%p3 bra 	$L__BB1_1;

	ret;

}

