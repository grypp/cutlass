//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32688072
// Cuda compilation tools, release 12.1, V12.1.105
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_90a
.address_size 64

	// .globl	_Z4testI6__halfS0_fEvPT_PT0_PT1_
.extern .shared .align 16 .b8 smem[];

.visible .entry _Z4testI6__halfS0_fEvPT_PT0_PT1_(
	.param .u64 _Z4testI6__halfS0_fEvPT_PT0_PT1__param_0,
	.param .u64 _Z4testI6__halfS0_fEvPT_PT0_PT1__param_1,
	.param .u64 _Z4testI6__halfS0_fEvPT_PT0_PT1__param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<133>;
	.reg .b32 	%r<86>;
	.reg .b64 	%rd<143>;


	ld.param.u64 	%rd3, [_Z4testI6__halfS0_fEvPT_PT0_PT1__param_0];
	ld.param.u64 	%rd4, [_Z4testI6__halfS0_fEvPT_PT0_PT1__param_1];
	ld.param.u64 	%rd5, [_Z4testI6__halfS0_fEvPT_PT0_PT1__param_2];
	mov.u32 	%r1, %tid.x;
	setp.gt.s32 	%p1, %r1, 1023;
	@%p1 bra 	$L__BB0_3;

	mov.u32 	%r2, %ntid.x;
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, smem;
	mov.u32 	%r84, %r1;

$L__BB0_2:
	mul.wide.s32 	%rd6, %r84, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs1, [%rd7];
	shl.b32 	%r8, %r84, 1;
	add.s32 	%r10, %r9, %r8;
	st.shared.u16 	[%r10], %rs1;
	add.s32 	%r84, %r84, %r2;
	setp.lt.s32 	%p2, %r84, 1024;
	@%p2 bra 	$L__BB0_2;

$L__BB0_3:
	setp.gt.s32 	%p3, %r1, 2047;
	@%p3 bra 	$L__BB0_6;

	mov.u32 	%r5, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd4;
	mov.u32 	%r12, smem;
	mov.u32 	%r85, %r1;

$L__BB0_5:
	mul.wide.s32 	%rd8, %r85, 2;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.u16 	%rs2, [%rd9];
	shl.b32 	%r11, %r85, 1;
	add.s32 	%r13, %r12, %r11;
	st.shared.u16 	[%r13+4096], %rs2;
	add.s32 	%r85, %r85, %r5;
	setp.lt.s32 	%p4, %r85, 2048;
	@%p4 bra 	$L__BB0_5;

$L__BB0_6:
	cvta.to.global.u64 	%rd12, %rd5;
	bar.sync 	0;
	mov.f32 	%f4, 0f00000000;
	mov.f32 	%f3, %f4;
	// begin inline asm
	// end inline asm
	// begin inline asm
	wgmma.fence.sync.aligned;

	// end inline asm
	mov.u32 	%r15, smem;
	shr.u32 	%r16, %r15, 4;
	and.b32  	%r17, %r16, 16383;
	cvt.u64.u32 	%rd13, %r17;
	or.b64  	%rd10, %rd13, 68720001024;
	add.s32 	%r18, %r15, 4096;
	shr.u32 	%r19, %r18, 4;
	and.b32  	%r20, %r19, 16383;
	cvt.u64.u32 	%rd14, %r20;
	or.b64  	%rd11, %rd14, 34368126976;
	mov.u32 	%r14, 0;
	mov.f32 	%f5, %f4;
	mov.f32 	%f6, %f4;
	mov.f32 	%f7, %f4;
	mov.f32 	%f8, %f4;
	mov.f32 	%f9, %f4;
	mov.f32 	%f10, %f4;
	mov.f32 	%f11, %f4;
	mov.f32 	%f12, %f4;
	mov.f32 	%f13, %f4;
	mov.f32 	%f14, %f4;
	mov.f32 	%f15, %f4;
	mov.f32 	%f16, %f4;
	mov.f32 	%f17, %f4;
	mov.f32 	%f18, %f4;
	mov.f32 	%f19, %f4;
	mov.f32 	%f20, %f4;
	mov.f32 	%f21, %f4;
	mov.f32 	%f22, %f4;
	mov.f32 	%f23, %f4;
	mov.f32 	%f24, %f4;
	mov.f32 	%f25, %f4;
	mov.f32 	%f26, %f4;
	mov.f32 	%f27, %f4;
	mov.f32 	%f28, %f4;
	mov.f32 	%f29, %f4;
	mov.f32 	%f30, %f4;
	mov.f32 	%f31, %f4;
	mov.f32 	%f32, %f4;
	mov.f32 	%f33, %f4;
	mov.f32 	%f34, %f4;
	mov.f32 	%f35, %f4;
	mov.f32 	%f36, %f4;
	mov.f32 	%f37, %f4;
	mov.f32 	%f38, %f4;
	mov.f32 	%f39, %f4;
	mov.f32 	%f40, %f4;
	mov.f32 	%f41, %f4;
	mov.f32 	%f42, %f4;
	mov.f32 	%f43, %f4;
	mov.f32 	%f44, %f4;
	mov.f32 	%f45, %f4;
	mov.f32 	%f46, %f4;
	mov.f32 	%f47, %f4;
	mov.f32 	%f48, %f4;
	mov.f32 	%f49, %f4;
	mov.f32 	%f50, %f4;
	mov.f32 	%f51, %f4;
	mov.f32 	%f52, %f4;
	mov.f32 	%f53, %f4;
	mov.f32 	%f54, %f4;
	mov.f32 	%f55, %f4;
	mov.f32 	%f56, %f4;
	mov.f32 	%f57, %f4;
	mov.f32 	%f58, %f4;
	mov.f32 	%f59, %f4;
	mov.f32 	%f60, %f4;
	mov.f32 	%f61, %f4;
	mov.f32 	%f62, %f4;
	mov.f32 	%f63, %f4;
	mov.f32 	%f64, %f4;
	mov.f32 	%f65, %f4;
	mov.f32 	%f66, %f4;
	// begin inline asm
	{
.reg .pred p;
setp.ne.b32 p, %r14, 0;
wgmma.mma_async.sync.aligned.m64n128k16.f32.f16.f16 {%f3,   %f4,   %f5,   %f6,   %f7,   %f8,   %f9,   %f10,    %f11,   %f12,   %f13,  %f14,  %f15,  %f16,  %f17,  %f18,   %f19,  %f20,  %f21,  %f22,  %f23,  %f24,  %f25,  %f26,   %f27,  %f28,  %f29,  %f30,  %f31,  %f32,  %f33,  %f34,   %f35,  %f36,  %f37,  %f38,  %f39,  %f40,  %f41,  %f42,   %f43,  %f44,  %f45,  %f46,  %f47,  %f48,  %f49,  %f50,   %f51,  %f52,  %f53,  %f54,  %f55,  %f56,  %f57,  %f58,   %f59,  %f60,  %f61,  %f62,  %f63,  %f64,  %f65,  %f66}, %rd10, %rd11, p,    1,  1,  0,  1;
}

	// end inline asm
	// begin inline asm
	wgmma.commit_group.sync.aligned;

	// end inline asm
	// begin inline asm
	wgmma.wait_group.sync.aligned 0;

	// end inline asm
	// begin inline asm
	// end inline asm
	mul.wide.u32 	%rd15, %r1, 4;
	add.s64 	%rd16, %rd12, %rd15;
	add.s32 	%r21, %r1, 128;
	mul.wide.u32 	%rd17, %r21, 4;
	add.s64 	%rd18, %rd12, %rd17;
	st.global.f32 	[%rd16], %f3;
	st.global.f32 	[%rd18], %f4;
	add.s32 	%r22, %r1, 256;
	mul.wide.u32 	%rd19, %r22, 4;
	add.s64 	%rd20, %rd12, %rd19;
	st.global.f32 	[%rd20], %f5;
	add.s32 	%r23, %r1, 384;
	mul.wide.u32 	%rd21, %r23, 4;
	add.s64 	%rd22, %rd12, %rd21;
	st.global.f32 	[%rd22], %f6;
	add.s32 	%r24, %r1, 512;
	mul.wide.u32 	%rd23, %r24, 4;
	add.s64 	%rd24, %rd12, %rd23;
	st.global.f32 	[%rd24], %f7;
	add.s32 	%r25, %r1, 640;
	mul.wide.u32 	%rd25, %r25, 4;
	add.s64 	%rd26, %rd12, %rd25;
	st.global.f32 	[%rd26], %f8;
	add.s32 	%r26, %r1, 768;
	mul.wide.u32 	%rd27, %r26, 4;
	add.s64 	%rd28, %rd12, %rd27;
	st.global.f32 	[%rd28], %f9;
	add.s32 	%r27, %r1, 896;
	mul.wide.u32 	%rd29, %r27, 4;
	add.s64 	%rd30, %rd12, %rd29;
	st.global.f32 	[%rd30], %f10;
	add.s32 	%r28, %r1, 1024;
	mul.wide.u32 	%rd31, %r28, 4;
	add.s64 	%rd32, %rd12, %rd31;
	st.global.f32 	[%rd32], %f11;
	add.s32 	%r29, %r1, 1152;
	mul.wide.u32 	%rd33, %r29, 4;
	add.s64 	%rd34, %rd12, %rd33;
	st.global.f32 	[%rd34], %f12;
	add.s32 	%r30, %r1, 1280;
	mul.wide.u32 	%rd35, %r30, 4;
	add.s64 	%rd36, %rd12, %rd35;
	st.global.f32 	[%rd36], %f13;
	add.s32 	%r31, %r1, 1408;
	mul.wide.u32 	%rd37, %r31, 4;
	add.s64 	%rd38, %rd12, %rd37;
	st.global.f32 	[%rd38], %f14;
	add.s32 	%r32, %r1, 1536;
	mul.wide.u32 	%rd39, %r32, 4;
	add.s64 	%rd40, %rd12, %rd39;
	st.global.f32 	[%rd40], %f15;
	add.s32 	%r33, %r1, 1664;
	mul.wide.u32 	%rd41, %r33, 4;
	add.s64 	%rd42, %rd12, %rd41;
	st.global.f32 	[%rd42], %f16;
	add.s32 	%r34, %r1, 1792;
	mul.wide.u32 	%rd43, %r34, 4;
	add.s64 	%rd44, %rd12, %rd43;
	st.global.f32 	[%rd44], %f17;
	add.s32 	%r35, %r1, 1920;
	mul.wide.u32 	%rd45, %r35, 4;
	add.s64 	%rd46, %rd12, %rd45;
	st.global.f32 	[%rd46], %f18;
	add.s32 	%r36, %r1, 2048;
	mul.wide.u32 	%rd47, %r36, 4;
	add.s64 	%rd48, %rd12, %rd47;
	st.global.f32 	[%rd48], %f19;
	add.s32 	%r37, %r1, 2176;
	mul.wide.u32 	%rd49, %r37, 4;
	add.s64 	%rd50, %rd12, %rd49;
	st.global.f32 	[%rd50], %f20;
	add.s32 	%r38, %r1, 2304;
	mul.wide.u32 	%rd51, %r38, 4;
	add.s64 	%rd52, %rd12, %rd51;
	st.global.f32 	[%rd52], %f21;
	add.s32 	%r39, %r1, 2432;
	mul.wide.u32 	%rd53, %r39, 4;
	add.s64 	%rd54, %rd12, %rd53;
	st.global.f32 	[%rd54], %f22;
	add.s32 	%r40, %r1, 2560;
	mul.wide.u32 	%rd55, %r40, 4;
	add.s64 	%rd56, %rd12, %rd55;
	st.global.f32 	[%rd56], %f23;
	add.s32 	%r41, %r1, 2688;
	mul.wide.u32 	%rd57, %r41, 4;
	add.s64 	%rd58, %rd12, %rd57;
	st.global.f32 	[%rd58], %f24;
	add.s32 	%r42, %r1, 2816;
	mul.wide.u32 	%rd59, %r42, 4;
	add.s64 	%rd60, %rd12, %rd59;
	st.global.f32 	[%rd60], %f25;
	add.s32 	%r43, %r1, 2944;
	mul.wide.u32 	%rd61, %r43, 4;
	add.s64 	%rd62, %rd12, %rd61;
	st.global.f32 	[%rd62], %f26;
	add.s32 	%r44, %r1, 3072;
	mul.wide.u32 	%rd63, %r44, 4;
	add.s64 	%rd64, %rd12, %rd63;
	st.global.f32 	[%rd64], %f27;
	add.s32 	%r45, %r1, 3200;
	mul.wide.u32 	%rd65, %r45, 4;
	add.s64 	%rd66, %rd12, %rd65;
	st.global.f32 	[%rd66], %f28;
	add.s32 	%r46, %r1, 3328;
	mul.wide.u32 	%rd67, %r46, 4;
	add.s64 	%rd68, %rd12, %rd67;
	st.global.f32 	[%rd68], %f29;
	add.s32 	%r47, %r1, 3456;
	mul.wide.u32 	%rd69, %r47, 4;
	add.s64 	%rd70, %rd12, %rd69;
	st.global.f32 	[%rd70], %f30;
	add.s32 	%r48, %r1, 3584;
	mul.wide.u32 	%rd71, %r48, 4;
	add.s64 	%rd72, %rd12, %rd71;
	st.global.f32 	[%rd72], %f31;
	add.s32 	%r49, %r1, 3712;
	mul.wide.u32 	%rd73, %r49, 4;
	add.s64 	%rd74, %rd12, %rd73;
	st.global.f32 	[%rd74], %f32;
	add.s32 	%r50, %r1, 3840;
	mul.wide.u32 	%rd75, %r50, 4;
	add.s64 	%rd76, %rd12, %rd75;
	st.global.f32 	[%rd76], %f33;
	add.s32 	%r51, %r1, 3968;
	mul.wide.u32 	%rd77, %r51, 4;
	add.s64 	%rd78, %rd12, %rd77;
	st.global.f32 	[%rd78], %f34;
	add.s32 	%r52, %r1, 4096;
	mul.wide.u32 	%rd79, %r52, 4;
	add.s64 	%rd80, %rd12, %rd79;
	st.global.f32 	[%rd80], %f35;
	add.s32 	%r53, %r1, 4224;
	mul.wide.u32 	%rd81, %r53, 4;
	add.s64 	%rd82, %rd12, %rd81;
	st.global.f32 	[%rd82], %f36;
	add.s32 	%r54, %r1, 4352;
	mul.wide.u32 	%rd83, %r54, 4;
	add.s64 	%rd84, %rd12, %rd83;
	st.global.f32 	[%rd84], %f37;
	add.s32 	%r55, %r1, 4480;
	mul.wide.u32 	%rd85, %r55, 4;
	add.s64 	%rd86, %rd12, %rd85;
	st.global.f32 	[%rd86], %f38;
	add.s32 	%r56, %r1, 4608;
	mul.wide.u32 	%rd87, %r56, 4;
	add.s64 	%rd88, %rd12, %rd87;
	st.global.f32 	[%rd88], %f39;
	add.s32 	%r57, %r1, 4736;
	mul.wide.u32 	%rd89, %r57, 4;
	add.s64 	%rd90, %rd12, %rd89;
	st.global.f32 	[%rd90], %f40;
	add.s32 	%r58, %r1, 4864;
	mul.wide.u32 	%rd91, %r58, 4;
	add.s64 	%rd92, %rd12, %rd91;
	st.global.f32 	[%rd92], %f41;
	add.s32 	%r59, %r1, 4992;
	mul.wide.u32 	%rd93, %r59, 4;
	add.s64 	%rd94, %rd12, %rd93;
	st.global.f32 	[%rd94], %f42;
	add.s32 	%r60, %r1, 5120;
	mul.wide.u32 	%rd95, %r60, 4;
	add.s64 	%rd96, %rd12, %rd95;
	st.global.f32 	[%rd96], %f43;
	add.s32 	%r61, %r1, 5248;
	mul.wide.u32 	%rd97, %r61, 4;
	add.s64 	%rd98, %rd12, %rd97;
	st.global.f32 	[%rd98], %f44;
	add.s32 	%r62, %r1, 5376;
	mul.wide.u32 	%rd99, %r62, 4;
	add.s64 	%rd100, %rd12, %rd99;
	st.global.f32 	[%rd100], %f45;
	add.s32 	%r63, %r1, 5504;
	mul.wide.u32 	%rd101, %r63, 4;
	add.s64 	%rd102, %rd12, %rd101;
	st.global.f32 	[%rd102], %f46;
	add.s32 	%r64, %r1, 5632;
	mul.wide.u32 	%rd103, %r64, 4;
	add.s64 	%rd104, %rd12, %rd103;
	st.global.f32 	[%rd104], %f47;
	add.s32 	%r65, %r1, 5760;
	mul.wide.u32 	%rd105, %r65, 4;
	add.s64 	%rd106, %rd12, %rd105;
	st.global.f32 	[%rd106], %f48;
	add.s32 	%r66, %r1, 5888;
	mul.wide.u32 	%rd107, %r66, 4;
	add.s64 	%rd108, %rd12, %rd107;
	st.global.f32 	[%rd108], %f49;
	add.s32 	%r67, %r1, 6016;
	mul.wide.u32 	%rd109, %r67, 4;
	add.s64 	%rd110, %rd12, %rd109;
	st.global.f32 	[%rd110], %f50;
	add.s32 	%r68, %r1, 6144;
	mul.wide.u32 	%rd111, %r68, 4;
	add.s64 	%rd112, %rd12, %rd111;
	st.global.f32 	[%rd112], %f51;
	add.s32 	%r69, %r1, 6272;
	mul.wide.u32 	%rd113, %r69, 4;
	add.s64 	%rd114, %rd12, %rd113;
	st.global.f32 	[%rd114], %f52;
	add.s32 	%r70, %r1, 6400;
	mul.wide.u32 	%rd115, %r70, 4;
	add.s64 	%rd116, %rd12, %rd115;
	st.global.f32 	[%rd116], %f53;
	add.s32 	%r71, %r1, 6528;
	mul.wide.u32 	%rd117, %r71, 4;
	add.s64 	%rd118, %rd12, %rd117;
	st.global.f32 	[%rd118], %f54;
	add.s32 	%r72, %r1, 6656;
	mul.wide.u32 	%rd119, %r72, 4;
	add.s64 	%rd120, %rd12, %rd119;
	st.global.f32 	[%rd120], %f55;
	add.s32 	%r73, %r1, 6784;
	mul.wide.u32 	%rd121, %r73, 4;
	add.s64 	%rd122, %rd12, %rd121;
	st.global.f32 	[%rd122], %f56;
	add.s32 	%r74, %r1, 6912;
	mul.wide.u32 	%rd123, %r74, 4;
	add.s64 	%rd124, %rd12, %rd123;
	st.global.f32 	[%rd124], %f57;
	add.s32 	%r75, %r1, 7040;
	mul.wide.u32 	%rd125, %r75, 4;
	add.s64 	%rd126, %rd12, %rd125;
	st.global.f32 	[%rd126], %f58;
	add.s32 	%r76, %r1, 7168;
	mul.wide.u32 	%rd127, %r76, 4;
	add.s64 	%rd128, %rd12, %rd127;
	st.global.f32 	[%rd128], %f59;
	add.s32 	%r77, %r1, 7296;
	mul.wide.u32 	%rd129, %r77, 4;
	add.s64 	%rd130, %rd12, %rd129;
	st.global.f32 	[%rd130], %f60;
	add.s32 	%r78, %r1, 7424;
	mul.wide.u32 	%rd131, %r78, 4;
	add.s64 	%rd132, %rd12, %rd131;
	st.global.f32 	[%rd132], %f61;
	add.s32 	%r79, %r1, 7552;
	mul.wide.u32 	%rd133, %r79, 4;
	add.s64 	%rd134, %rd12, %rd133;
	st.global.f32 	[%rd134], %f62;
	add.s32 	%r80, %r1, 7680;
	mul.wide.u32 	%rd135, %r80, 4;
	add.s64 	%rd136, %rd12, %rd135;
	st.global.f32 	[%rd136], %f63;
	add.s32 	%r81, %r1, 7808;
	mul.wide.u32 	%rd137, %r81, 4;
	add.s64 	%rd138, %rd12, %rd137;
	st.global.f32 	[%rd138], %f64;
	add.s32 	%r82, %r1, 7936;
	mul.wide.u32 	%rd139, %r82, 4;
	add.s64 	%rd140, %rd12, %rd139;
	st.global.f32 	[%rd140], %f65;
	add.s32 	%r83, %r1, 8064;
	mul.wide.u32 	%rd141, %r83, 4;
	add.s64 	%rd142, %rd12, %rd141;
	st.global.f32 	[%rd142], %f66;
	ret;

}
	// .globl	_Z9referenceI6__halfS0_fEvPT_PT0_PT1_
.visible .entry _Z9referenceI6__halfS0_fEvPT_PT0_PT1_(
	.param .u64 _Z9referenceI6__halfS0_fEvPT_PT0_PT1__param_0,
	.param .u64 _Z9referenceI6__halfS0_fEvPT_PT0_PT1__param_1,
	.param .u64 _Z9referenceI6__halfS0_fEvPT_PT0_PT1__param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<65>;
	.reg .f32 	%f<34>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd5, [_Z9referenceI6__halfS0_fEvPT_PT0_PT1__param_0];
	ld.param.u64 	%rd6, [_Z9referenceI6__halfS0_fEvPT_PT0_PT1__param_1];
	ld.param.u64 	%rd7, [_Z9referenceI6__halfS0_fEvPT_PT0_PT1__param_2];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r11, 0;
	mov.u32 	%r2, %ntid.x;

$L__BB1_1:
	setp.gt.s32 	%p1, %r1, 127;
	@%p1 bra 	$L__BB1_4;

	shl.b32 	%r9, %r11, 4;
	shl.b32 	%r4, %r11, 7;
	mul.wide.s32 	%rd8, %r9, 2;
	add.s64 	%rd4, %rd3, %rd8;
	mov.u32 	%r12, %r1;

$L__BB1_3:
	add.s32 	%r10, %r12, %r4;
	mul.wide.s32 	%rd9, %r10, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u16 	%rs2, [%rd4];
	mul.wide.s32 	%rd11, %r12, 2;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.u16 	%rs3, [%rd12];
	// begin inline asm
	{mul.f16 %rs1,%rs2,%rs3;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// end inline asm
	ld.global.f32 	%f17, [%rd10];
	add.f32 	%f18, %f1, %f17;
	st.global.f32 	[%rd10], %f18;
	ld.global.u16 	%rs6, [%rd4+2];
	ld.global.u16 	%rs7, [%rd12+256];
	// begin inline asm
	{mul.f16 %rs5,%rs6,%rs7;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs5;}

	// end inline asm
	add.f32 	%f19, %f2, %f18;
	st.global.f32 	[%rd10], %f19;
	ld.global.u16 	%rs10, [%rd4+4];
	ld.global.u16 	%rs11, [%rd12+512];
	// begin inline asm
	{mul.f16 %rs9,%rs10,%rs11;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f3, %rs9;}

	// end inline asm
	add.f32 	%f20, %f3, %f19;
	st.global.f32 	[%rd10], %f20;
	ld.global.u16 	%rs14, [%rd4+6];
	ld.global.u16 	%rs15, [%rd12+768];
	// begin inline asm
	{mul.f16 %rs13,%rs14,%rs15;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs13;}

	// end inline asm
	add.f32 	%f21, %f4, %f20;
	st.global.f32 	[%rd10], %f21;
	ld.global.u16 	%rs18, [%rd4+8];
	ld.global.u16 	%rs19, [%rd12+1024];
	// begin inline asm
	{mul.f16 %rs17,%rs18,%rs19;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs17;}

	// end inline asm
	add.f32 	%f22, %f5, %f21;
	st.global.f32 	[%rd10], %f22;
	ld.global.u16 	%rs22, [%rd4+10];
	ld.global.u16 	%rs23, [%rd12+1280];
	// begin inline asm
	{mul.f16 %rs21,%rs22,%rs23;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f6, %rs21;}

	// end inline asm
	add.f32 	%f23, %f6, %f22;
	st.global.f32 	[%rd10], %f23;
	ld.global.u16 	%rs26, [%rd4+12];
	ld.global.u16 	%rs27, [%rd12+1536];
	// begin inline asm
	{mul.f16 %rs25,%rs26,%rs27;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs25;}

	// end inline asm
	add.f32 	%f24, %f7, %f23;
	st.global.f32 	[%rd10], %f24;
	ld.global.u16 	%rs30, [%rd4+14];
	ld.global.u16 	%rs31, [%rd12+1792];
	// begin inline asm
	{mul.f16 %rs29,%rs30,%rs31;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs29;}

	// end inline asm
	add.f32 	%f25, %f8, %f24;
	st.global.f32 	[%rd10], %f25;
	ld.global.u16 	%rs34, [%rd4+16];
	ld.global.u16 	%rs35, [%rd12+2048];
	// begin inline asm
	{mul.f16 %rs33,%rs34,%rs35;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f9, %rs33;}

	// end inline asm
	add.f32 	%f26, %f9, %f25;
	st.global.f32 	[%rd10], %f26;
	ld.global.u16 	%rs38, [%rd4+18];
	ld.global.u16 	%rs39, [%rd12+2304];
	// begin inline asm
	{mul.f16 %rs37,%rs38,%rs39;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs37;}

	// end inline asm
	add.f32 	%f27, %f10, %f26;
	st.global.f32 	[%rd10], %f27;
	ld.global.u16 	%rs42, [%rd4+20];
	ld.global.u16 	%rs43, [%rd12+2560];
	// begin inline asm
	{mul.f16 %rs41,%rs42,%rs43;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs41;}

	// end inline asm
	add.f32 	%f28, %f11, %f27;
	st.global.f32 	[%rd10], %f28;
	ld.global.u16 	%rs46, [%rd4+22];
	ld.global.u16 	%rs47, [%rd12+2816];
	// begin inline asm
	{mul.f16 %rs45,%rs46,%rs47;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f12, %rs45;}

	// end inline asm
	add.f32 	%f29, %f12, %f28;
	st.global.f32 	[%rd10], %f29;
	ld.global.u16 	%rs50, [%rd4+24];
	ld.global.u16 	%rs51, [%rd12+3072];
	// begin inline asm
	{mul.f16 %rs49,%rs50,%rs51;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs49;}

	// end inline asm
	add.f32 	%f30, %f13, %f29;
	st.global.f32 	[%rd10], %f30;
	ld.global.u16 	%rs54, [%rd4+26];
	ld.global.u16 	%rs55, [%rd12+3328];
	// begin inline asm
	{mul.f16 %rs53,%rs54,%rs55;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs53;}

	// end inline asm
	add.f32 	%f31, %f14, %f30;
	st.global.f32 	[%rd10], %f31;
	ld.global.u16 	%rs58, [%rd4+28];
	ld.global.u16 	%rs59, [%rd12+3584];
	// begin inline asm
	{mul.f16 %rs57,%rs58,%rs59;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f15, %rs57;}

	// end inline asm
	add.f32 	%f32, %f15, %f31;
	st.global.f32 	[%rd10], %f32;
	ld.global.u16 	%rs62, [%rd4+30];
	ld.global.u16 	%rs63, [%rd12+3840];
	// begin inline asm
	{mul.f16 %rs61,%rs62,%rs63;
}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs61;}

	// end inline asm
	add.f32 	%f33, %f16, %f32;
	st.global.f32 	[%rd10], %f33;
	add.s32 	%r12, %r12, %r2;
	setp.lt.s32 	%p2, %r12, 128;
	@%p2 bra 	$L__BB1_3;

$L__BB1_4:
	add.s32 	%r11, %r11, 1;
	setp.lt.u32 	%p3, %r11, 64;
	@%p3 bra 	$L__BB1_1;

	ret;

}

